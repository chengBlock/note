# 爬虫初识

- 什么是爬虫

  网络爬虫,是一种按照一定规则,自动抓取互联网信息的程序或者脚本。由于互联网数据的多样性和资源的有限性，根据用户需求定向抓取相关网页并分析已成为如今主流的爬取策略。
  
- 爬虫的本质

  模拟浏览器打开网页，获取网页中我们想要的那部分数据。

# 基本流程

1. 准备工作

   通过浏览器查看分析目标网页，学习编程基础规范。

2. 获取数据

   通过HTTP库向目标站点发起请求，请求可以包含额外的header等信息，如果服务器能够正常响应，会得到一个Response，便是所要获取的页面内容。

3. 解析内容

   得到的内容可能是HTML、json等格式，可以用页面解析库、正则表达式等进行解析。

4. 保持数据

   保存形式多样，可以存为文本，也可以保存到数据库，或者保持特定格式的文件。